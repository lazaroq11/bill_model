{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1fqDQn9VMIcT7vjLJVm_SJNca1XS5sXtW",
      "authorship_tag": "ABX9TyP0yj1O7wW222pjKBK/LB/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lazaroq11/bill_model/blob/master/bill_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Mf7UOmDJOap",
        "outputId": "f462d3fe-b92a-4a83-aa0e-bd12059e2a94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install -U scikit-learn\n",
        "!pip install pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3yJigg1Hq_f",
        "outputId": "c9deb2b1-cdd8-458b-c19f-674bee5bba39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR8OTL7sLBtT",
        "outputId": "fd2a7cbd-d653-452d-89f4-888bcec656ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (17.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "Successfully installed accelerate-0.23.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orXX5oHAX8sc",
        "outputId": "f5f94508-3fa4-412f-bb42-4e98fe254ffc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBLQLQIFHpTZ",
        "outputId": "73e90b6d-9a2c-4899-fad7-8157bef54d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 5.245918154716492\n",
            "Epoch 2, Loss: 1.9514864087104797\n",
            "Epoch 3, Loss: 1.2550402283668518\n",
            "Epoch 4, Loss: 0.8739748299121857\n",
            "Epoch 5, Loss: 0.5643481314182281\n",
            "Epoch 6, Loss: 0.3487187922000885\n",
            "Epoch 7, Loss: 0.20729544758796692\n",
            "Epoch 8, Loss: 0.12430823594331741\n",
            "Epoch 9, Loss: 0.061371784657239914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.04847209341824055\n",
            "Texto gerado na época 10:\n",
            "BILL ACSO (Núcleo de Arquitetura de Computadores e Sistemas Operacionais), é um grupo de Pesquisa district_attorney UNEB, Universidade do Estado da Bahia\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine_tuned_gpt2/tokenizer_config.json',\n",
              " 'fine_tuned_gpt2/special_tokens_map.json',\n",
              " 'fine_tuned_gpt2/vocab.json',\n",
              " 'fine_tuned_gpt2/merges.txt',\n",
              " 'fine_tuned_gpt2/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Seu DataFrame com textos e rótulos\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from textblob import Word\n",
        "\n",
        "# Seu DataFrame com textos e rótulos\n",
        "df = pd.DataFrame({\n",
        "    \"label\": [\"BILL\", \"BahiaRT\", \"python\",\"AI\", \"Machine Learning\", \"NLP\", \"ACSO\", \"UNEB\", \"BahiaRt Multidisciplinar\",\"Yolo\", \"ROS\"],\n",
        "    \"text\": [\"BILL é um robô de serviço projetado pelo teamBahiaRt. Ele é um robô humanoide que pode ser usado para uma variedade de tarefas, incluindo atendimento ao cliente, limpeza e segurança. Bill é equipado com uma variedade de sensores e câmeras, que ele usa para navegar pelo ambiente e interagir com humanos.Bill é um produto da BahiaRT, uma equipe de robótica da UNEB. A BahiaRT é uma equipe de estudantes e pesquisadores que trabalham no desenvolvimento de robôs para aplicações sociais. Bill é um dos projetos mais recentes da BahiaRT.Bill é um robô inovador que tem o potencial de melhorar a vida das pessoas. Ele pode ser usado para automatizar tarefas, fornecer assistência aos idosos e deficientes e até mesmo ajudar a salvar vidas.\",\n",
        "              \"BahiaRT é uma equipe de robótica da UNEB. A equipe foi fundada em 2015 por um grupo de estudantes e pesquisadores que compartilham o interesse em robótica.\",\n",
        "              \"Python é uma linguagem de programação geral de alto nível. É uma linguagem interpretada, o que significa que não precisa ser compilada antes de ser executada. Python é uma linguagem relativamente fácil de aprender e usar.\",\n",
        "              \"A inteligência artificial (IA) é uma área da ciência da computação que se concentra no desenvolvimento de sistemas capazes de executar tarefas que normalmente requerem inteligência humana. Ela engloba campos como aprendizado de máquina, visão computacional e processamento de linguagem natural.\",\n",
        "              \"A aprendizagem de máquina (Machine Learning) é um subcampo da inteligência artificial que se concentra no desenvolvimento de algoritmos e modelos que permitem que os sistemas aprendam e melhorem com base em dados. É amplamente utilizado em previsões e análise de dados.\",\n",
        "              \"O Processamento de Linguagem Natural (NLP) é uma subárea da IA que se concentra na interação entre computadores e linguagem humana. É usado em chatbots, tradução automática e análise de sentimentos, entre outras aplicações.\",\n",
        "              \"O ACSO (Núcleo de Arquitetura de Computadores e Sistemas Operacionais), é um grupo de Pesquisa da UNEB, Universidade do Estado da Bahia\",\n",
        "              \"A Uneb é uma das maiores universidades da Bahia, multicampi e para todos.\",\n",
        "              \"A BahiaRT é uma equipe multidisciplinar que inclui estudantes de engenharia, ciência da computação e design.A BahiaRT tem trabalhado em uma variedade de projetos de robótica, incluindo Bill, um robô de serviço humanoide. A equipe também está trabalhando em um robô de resgate que pode ser usado em situações de emergência.A BahiaRT é uma equipe ativa que está constantemente desenvolvendo novos projetos de robótica. A equipe está comprometida em usar a robótica para melhorar a vida das pessoas.\",\n",
        "              \"O Yolo é um algoritmo de AI, utilizado para detecção de pessoas e objetos, BILL utiliza esse algoritmo para executar tarefas relacionadas a isso\",\n",
        "              \"Robot Operating System é uma coleção de frameworks de software para desenvolvimento de robôs, que fornece a funcionalidade de um sistema operacional em um cluster de computadores heterogêneo \"]\n",
        "})\n",
        "\n",
        "# Função para substituir palavras por sinônimos\n",
        "def replace_with_synonyms(text):\n",
        "    words = text.split()\n",
        "    for i in range(len(words)):\n",
        "        if random.random() < 0.2:  # Probabilidade de 20% de substituição\n",
        "            # Obter sinônimos da palavra usando Wordnet\n",
        "            synonyms = wordnet.synsets(words[i])\n",
        "            if synonyms:\n",
        "                new_word = synonyms[0].lemmas()[0].name()\n",
        "                words[i] = new_word\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Aplicar a função de substituição de palavras por sinônimos ao DataFrame\n",
        "df['text'] = df['text'].apply(replace_with_synonyms)\n",
        "\n",
        "# Dividir o DataFrame em treinamento e teste\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# Carregar o tokenizador e modelo pré-treinado GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "\n",
        "# Defina um token de padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Defina o tamanho do lote e crie um DataLoader personalizado para treinamento\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.texts = df['text'].values\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizer(self.texts[idx], return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
        "        return inputs\n",
        "\n",
        "train_dataset = TextDataset(train_df, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Configurar otimizador e agendador de taxa de aprendizado\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 10)\n",
        "\n",
        "# Adicione dropout como técnica de regularização\n",
        "model.config.dropout = 0.1\n",
        "\n",
        "# Treinamento\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    total_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch[\"input_ids\"]\n",
        "        loss = model(inputs, labels=inputs).loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}')\n",
        "\n",
        "    # Gere texto de exemplo após algumas épocas\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval()\n",
        "        prompt = \"BILL\"\n",
        "        prompt_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "        generated_text = model.generate(prompt_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "        print(f'Texto gerado na época {epoch + 1}:')\n",
        "        print(tokenizer.decode(generated_text[0], skip_special_tokens=True))\n",
        "        model.train()\n",
        "\n",
        "# Salvar o modelo fine-tunado\n",
        "model.save_pretrained(\"fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ty1_H5GmmldF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Carregar o modelo fine-tunado\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"fine_tuned_gpt2\")\n",
        "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"fine_tuned_gpt2\")\n",
        "\n",
        "# Exemplo de inferência com temperatura definida\n",
        "prompt = \"O que é o ACSO\"\n",
        "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors='pt')\n",
        "temperature = 0.6  # Ajuste o valor de temperatura conforme desejado\n",
        "with torch.no_grad():\n",
        "    output = fine_tuned_model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, temperature=temperature)\n",
        "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Texto gerado:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRpTZ5WjR-Fo",
        "outputId": "0bd86752-0b0e-4d91-b1f1-7ce2fc4c71f6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto gerado:\n",
            "O que é o ACSO (Núcleo de Arquitetura de Computadores e Sistemas Operacionais), é um grupo de Pesquisa district_attorney UNEB, Universidade do Estado da Bahia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiWRcSAI4IFI",
        "outputId": "35d7a519-e355-4092-e11d-11a48e6bf2e4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from textblob import Word\n",
        "\n",
        "# Seu DataFrame com textos e rótulos\n",
        "df = pd.DataFrame({\n",
        "    \"label\": [\"BILL\", \"BahiaRT\", \"python\",\"AI\", \"Machine Learning\", \"NLP\", \"ACSO\", \"UNEB\", \"BahiaRt Multidisciplinar\",\"Yolo\", \"ROS\"],\n",
        "    \"text\": [\"BILL é um robô de serviço projetado pelo teamBahiaRt. Ele é um robô humanoide que pode ser usado para uma variedade de tarefas, incluindo atendimento ao cliente, limpeza e segurança. Bill é equipado com uma variedade de sensores e câmeras, que ele usa para navegar pelo ambiente e interagir com humanos.Bill é um produto da BahiaRT, uma equipe de robótica da UNEB. A BahiaRT é uma equipe de estudantes e pesquisadores que trabalham no desenvolvimento de robôs para aplicações sociais. Bill é um dos projetos mais recentes da BahiaRT.Bill é um robô inovador que tem o potencial de melhorar a vida das pessoas. Ele pode ser usado para automatizar tarefas, fornecer assistência aos idosos e deficientes e até mesmo ajudar a salvar vidas.\",\n",
        "              \"BahiaRT é uma equipe de robótica da UNEB. A equipe foi fundada em 2015 por um grupo de estudantes e pesquisadores que compartilham o interesse em robótica.\",\n",
        "              \"Python é uma linguagem de programação geral de alto nível. É uma linguagem interpretada, o que significa que não precisa ser compilada antes de ser executada. Python é uma linguagem relativamente fácil de aprender e usar.\",\n",
        "              \"A inteligência artificial (IA) é uma área da ciência da computação que se concentra no desenvolvimento de sistemas capazes de executar tarefas que normalmente requerem inteligência humana. Ela engloba campos como aprendizado de máquina, visão computacional e processamento de linguagem natural.\",\n",
        "              \"A aprendizagem de máquina (Machine Learning) é um subcampo da inteligência artificial que se concentra no desenvolvimento de algoritmos e modelos que permitem que os sistemas aprendam e melhorem com base em dados. É amplamente utilizado em previsões e análise de dados.\",\n",
        "              \"O Processamento de Linguagem Natural (NLP) é uma subárea da IA que se concentra na interação entre computadores e linguagem humana. É usado em chatbots, tradução automática e análise de sentimentos, entre outras aplicações.\",\n",
        "              \"O ACSO (Núcleo de Arquitetura de Computadores e Sistemas Operacionais), é um grupo de Pesquisa da UNEB, Universidade do Estado da Bahia\",\n",
        "              \"A Uneb é uma das maiores universidades da Bahia, multicampi e para todos.\",\n",
        "              \"A BahiaRT é uma equipe multidisciplinar que inclui estudantes de engenharia, ciência da computação e design.A BahiaRT tem trabalhado em uma variedade de projetos de robótica, incluindo Bill, um robô de serviço humanoide. A equipe também está trabalhando em um robô de resgate que pode ser usado em situações de emergência.A BahiaRT é uma equipe ativa que está constantemente desenvolvendo novos projetos de robótica. A equipe está comprometida em usar a robótica para melhorar a vida das pessoas.\",\n",
        "              \"O Yolo é um algoritmo de AI, utilizado para detecção de pessoas e objetos, BILL utiliza esse algoritmo para executar tarefas relacionadas a isso\",\n",
        "              \"Robot Operating System é uma coleção de frameworks de software para desenvolvimento de robôs, que fornece a funcionalidade de um sistema operacional em um cluster de computadores heterogêneo \"]\n",
        "})\n",
        "\n",
        "# Função para substituir palavras por sinônimos\n",
        "def replace_with_synonyms(text):\n",
        "    words = text.split()\n",
        "    for i in range(len(words)):\n",
        "        if random.random() < 0.2:  # Probabilidade de 20% de substituição\n",
        "            # Obter sinônimos da palavra usando Wordnet\n",
        "            synonyms = wordnet.synsets(words[i])\n",
        "            if synonyms:\n",
        "                new_word = synonyms[0].lemmas()[0].name()\n",
        "                words[i] = new_word\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Aplicar a função de substituição de palavras por sinônimos ao DataFrame\n",
        "df['text'] = df['text'].apply(replace_with_synonyms)\n",
        "\n",
        "print(df['text'])\n",
        "\n",
        "# Agora, o DataFrame contém amostras com palavras substituídas por sinônimos\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7R93DoG36ma",
        "outputId": "b2e150e7-b9a1-4152-cc4b-702300b288ed"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     bill é um robô Delaware serviço projetado pelo...\n",
            "1     BahiaRT é uma equipe de robótica da UNEB. A eq...\n",
            "2     Python é uma linguagem de programação geral de...\n",
            "3     A inteligência artificial (IA) é uma área da c...\n",
            "4     A aprendizagem de máquina (Machine Learning) é...\n",
            "5     O Processamento de Linguagem Natural (NLP) é u...\n",
            "6     O ACSO (Núcleo de Arquitetura de Computadores ...\n",
            "7     A Uneb é uma das maiores universidades distric...\n",
            "8     A BahiaRT é uma equipe multidisciplinar que in...\n",
            "9     O Yolo é um algoritmo Delaware AI, utilizado p...\n",
            "10    Robot Operating System é uma coleção de framew...\n",
            "Name: text, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}